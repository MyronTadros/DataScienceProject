{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><em>Student Academic Performance Prediction:</em></strong><br>\n",
    "***\n",
    "This notebook attempts to study some of the factors affecting student academic success, the first measurement of success being the GPA and the second being eligibility for application of a master's degree\n",
    "\n",
    "Features being studied are:<br>\n",
    "<ul> \n",
    "<li>SAT score</li>\n",
    "<li>Athletism</li>\n",
    "<li>High School Size</li>\n",
    "<li>High School Rank</li>\n",
    "<li>High School Percentile</li>\n",
    "<li>Gender</li>\n",
    "</ul>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h3>Libraries Import</h3>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNCOMMENT THE CODE BELOW ONLY IF YOU ARE IMPORTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install statsmodels\n",
    "%pip install dash\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import Ridge , LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import statsmodels.api as sm\n",
    "from dash import dash, dcc, html, Input, Output\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Import</h3>\n",
    "<p>The main dataframes are:</p> \n",
    "<ol>\n",
    "<li>merged_df</li>\n",
    "<li>unchanged_df</li>\n",
    "</ol>\n",
    "<p>Where <strong>merged_df</strong> is the dataframe we're going to experiment on </p>\n",
    "<p>And <strong>unchaged_df</strong> is the dataframe used for the plots</p>\n",
    "<p> A <strong> \"Success\" </strong> attribute was added to represent student ability to enroll for a masters </p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df= pd.read_csv('GPA2.csv')\n",
    "unchanged_df = pd.read_csv('GPA2.csv') # For the \"before\" graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a \"success\" column\n",
    "success =[]\n",
    "successCount = 0;\n",
    "failCount = 0;\n",
    "for i in merged_df[\"gpa\"]:\n",
    "    if(i >= (3)):\n",
    "        success.append(True)\n",
    "        successCount+= 1\n",
    "    else:\n",
    "        success.append(False)\n",
    "        failCount+=1\n",
    "\n",
    "merged_df.insert(7,\"Success\", success);\n",
    "print(\"Succes Count is: \", successCount)\n",
    "print(\"Fail Count is: \", failCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <h3>Exploratory Data Analysis <strong>EDA</strong></h3>\n",
    "    <p>This part includes:</p>\n",
    "    <ul>\n",
    "        <li>Overview of the data</li>\n",
    "        <li>Graphs for the data</li>\n",
    "        <li>A dashboard</li>\n",
    "    </ul>\n",
    "    <p>Ambiguous or unclear graphs will be explained in the comments.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unchanged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unchanged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first check for duplicates:\n",
    "print(\"Duplicates in data frame:\\n\",unchanged_df[unchanged_df.duplicated()])\n",
    "# we found a duplicate in the df, it's student number 533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df= merged_df.drop_duplicates()\n",
    "#here we saved the merged_df without the duplicate\n",
    "merged_df.info()\n",
    "#we can notice that the data is now one entry less. (it was 4135 and now is 4134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this again after the duplicate is gone\n",
    "merged_df.describe()\n",
    "#we can also conclude a few things from this. for example, nearly half of these students are female, \n",
    "#and almost 5% only are athletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define layout\n",
    "app.layout = html.Div(style={'backgroundColor': '#f7f7f7', 'padding': '20px'}, children=[\n",
    "    html.H1(\"Data Visualization Dashboard\", style={'textAlign': 'center', 'margin-bottom': '20px', 'color': '#333'}),\n",
    "    \n",
    "    # Plot type selector\n",
    "    html.Div([\n",
    "        html.Label(\"Select Plot Type\", style={'font-weight': 'bold', 'color': '#555'}),\n",
    "        dcc.RadioItems(\n",
    "            id='plot-type',\n",
    "            options=[\n",
    "                {'label': 'Scatter Plot', 'value': 'scatter'},\n",
    "                {'label': 'Bar Plot', 'value': 'bar'},\n",
    "                {'label': 'Box Plot', 'value': 'box'},\n",
    "                {'label': 'Histogram', 'value': 'histogram'},\n",
    "                {'label': 'Correlation Matrix', 'value': 'correlation'}\n",
    "            ],\n",
    "            value='scatter',  # Default value\n",
    "            labelStyle={'display': 'block'},\n",
    "            style={'margin-top': '10px'}\n",
    "        ),\n",
    "    ], id='controls', style={'width': '50%', 'margin': 'auto', 'textAlign': 'center'}),\n",
    "    \n",
    "    # Dropdowns\n",
    "    html.Div(id='x-variable-div', children=[\n",
    "        html.Label(\"Select X Variable\", style={'font-weight': 'bold', 'color': '#555'}),\n",
    "        dcc.Dropdown(\n",
    "            id='x-variable',\n",
    "            options=[{'label': col, 'value': col} for col in unchanged_df.columns[1:] if col not in ['athlete', 'female']],\n",
    "            value=unchanged_df.columns[1],  # Default value\n",
    "            clearable=False,\n",
    "            style={'margin-top': '10px'}\n",
    "        ),\n",
    "    ], style={'textAlign': 'center', 'display': 'none', 'margin-top': '20px'}),\n",
    "    \n",
    "    html.Div(id='box-variable-div', children=[\n",
    "        html.Label(\"Select Box Variable\", style={'font-weight': 'bold', 'color': '#555'}),\n",
    "        dcc.Dropdown(\n",
    "            id='box-variable',\n",
    "            options=[\n",
    "                {'label': 'Female', 'value': 'female'},\n",
    "                {'label': 'Athlete', 'value': 'athlete'}\n",
    "            ],\n",
    "            value='female',\n",
    "            clearable=False,\n",
    "            style={'margin-top': '10px'}\n",
    "        ),\n",
    "    ], style={'textAlign': 'center', 'display': 'none', 'margin-top': '20px'}),\n",
    "    \n",
    "    # Graph\n",
    "    dcc.Graph(id='graph', style={'margin-top': '20px'})\n",
    "])\n",
    "\n",
    "# Callbacks\n",
    "@app.callback(\n",
    "    [Output('x-variable-div', 'style'), Output('box-variable-div', 'style')],\n",
    "    Input('plot-type', 'value')\n",
    ")\n",
    "def show_hide_dropdowns(plot_type):\n",
    "    x_style = {'display': 'none'}\n",
    "    box_style = {'display': 'none'}\n",
    "    \n",
    "    if plot_type in ['scatter', 'bar']:\n",
    "        x_style = {'display': 'block'}\n",
    "    elif plot_type == 'box':\n",
    "        box_style = {'display': 'block'}\n",
    "    \n",
    "    return x_style, box_style\n",
    "\n",
    "@app.callback(\n",
    "    Output('graph', 'figure'),\n",
    "    [Input('x-variable', 'value'),\n",
    "     Input('plot-type', 'value'),\n",
    "     Input('box-variable', 'value')]\n",
    ")\n",
    "def update_graph(x_variable, plot_type, box_variable):\n",
    "    if plot_type == 'scatter':\n",
    "        fig = px.scatter(unchanged_df, x=x_variable, y='gpa', title=f'GPA vs {x_variable}', labels={'gpa':'GPA Score'})\n",
    "    elif plot_type == 'bar':\n",
    "        fig = px.bar(unchanged_df, x=x_variable, y='gpa', title=f'GPA vs {x_variable}', labels={'gpa':'GPA Score'})\n",
    "    elif plot_type == 'box':\n",
    "        fig = px.box(unchanged_df, x=box_variable, y='gpa', title=f'GPA vs {box_variable.capitalize()}', labels={'gpa':'GPA Score'})\n",
    "    elif plot_type == 'histogram':\n",
    "        bin_num = int(2 * np.cbrt(unchanged_df.size))  # Convert bin_num to an integer\n",
    "        fig = px.histogram(unchanged_df, x='gpa', title='Histogram of GPA', labels={'gpa': 'GPA Score'}, nbins=bin_num)\n",
    "    elif plot_type == 'correlation':\n",
    "        correlation_matrix = unchanged_df[['gpa', 'SAT', 'athlete', 'hsize', 'hsrank', 'hsperc', 'female']].corr()\n",
    "        correlation_matrix_text = correlation_matrix.round(2).astype(str)\n",
    "        fig = go.Figure(data=go.Heatmap(x=correlation_matrix.columns, z=correlation_matrix.values, y=correlation_matrix.columns,\n",
    "            colorscale='RdBu_r', zmin=-1, zmax=1,text=correlation_matrix_text.values,hoverinfo='text'))\n",
    "        fig.update_traces(text=correlation_matrix_text.values, texttemplate=\"%{text}\")\n",
    "        fig.update_layout(title='Correlation Matrix', xaxis_nticks=36)\n",
    "    return fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code is probably going to bye bye\n",
    "# Generate the correlation matrix\n",
    "correlation_matrix = merged_df[['gpa', 'SAT', 'athlete','hsize','hsrank','hsperc','female']].corr()\n",
    "\n",
    "# Generate a heatmap for the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<strong><em>Initial Test Using All of The Dataset:</em></strong><br>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data, we decided to drop hsperc as it is equal to rank/size --> dependent variable\n",
    "X = merged_df[['SAT', 'athlete','hsize','hsrank','female']]\n",
    "y = merged_df['gpa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal LinearRegression with k = 3 fold cross validation\n",
    "\n",
    "best_mse = 17 #arbitrarly large MSE, (MSE for 0% accuracy is at maximum 16)\n",
    "best_model = 0\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=3, random_state=None)\n",
    "total_test_mse = 0\n",
    "total_train_mse = 0\n",
    "    \n",
    "for j, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "    # Train Test split\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "\n",
    "    # Normalization\n",
    "    mean_X_train = np.mean(X_train, axis=0)\n",
    "    std_X_train = np.std(X_train, axis=0)\n",
    "    mean_y_train = np.mean(y_train)\n",
    "    std_y_train = np.std(y_train)\n",
    "\n",
    "\n",
    "    X_train_norm = (X_train - mean_X_train)/std_X_train\n",
    "    X_test_norm = (X_test - mean_X_train)/std_X_train\n",
    "\n",
    "    y_train_norm = (y_train - mean_y_train)/std_y_train\n",
    "    y_test_norm = (y_test - mean_y_train)/std_y_train\n",
    "\n",
    "    # Linear Regression\n",
    "    model = LinearRegression(fit_intercept= True)\n",
    "    model.fit(X_train_norm, y_train_norm)\n",
    "\n",
    "    # Train and test MSE\n",
    "    y_train_predict_unnormalized = (model.predict(X_train_norm) * std_y_train) + mean_y_train\n",
    "    y_test_predict_unnormalized = (model.predict(X_test_norm) * std_y_train) + mean_y_train\n",
    "\n",
    "    train_mse = np.mean((y_train_predict_unnormalized - y_train) ** 2)\n",
    "    test_mse = np.mean((y_test_predict_unnormalized - y_test) ** 2)\n",
    "    total_train_mse += train_mse\n",
    "    total_test_mse += test_mse\n",
    "\n",
    "    if (test_mse < best_mse): #Choose the model with lowest MSE for hypothesis testing and R2 Calculation\n",
    "        best_mse = test_mse\n",
    "        best_model = model\n",
    "\n",
    "print(\"Average Train MSE:\", total_train_mse / kf.get_n_splits())\n",
    "print(\"Average Test MSE:\", total_test_mse / kf.get_n_splits())\n",
    "print(\"Best model coeffecients: \", best_model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÙŒR2 Score Calculation\n",
    "\n",
    "X_all_norm = (X - mean_X_train)/std_X_train\n",
    "y_pred_all = best_model.predict(X_all_norm)\n",
    "y_pred_all_unnormalized = (y_pred_all * std_y_train) + mean_y_train\n",
    "\n",
    "print (\"The r2 score is: \", r2_score(y,y_pred_all_unnormalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis testing on the least error model\n",
    "\n",
    "X_const_added = sm.add_constant(X_all_norm)\n",
    "est = sm.OLS(y,X_all_norm)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a sample of 500 students\n",
    "\n",
    "\n",
    "numss = [np.random.randint(1,len(y)) for _ in range(500)]\n",
    "y_rand = y.iloc[numss]\n",
    "y_pred_rand = y_pred_all_unnormalized[numss]\n",
    "\n",
    "# Assuming y_pred_all_unnormalized and y are numpy arrays of the same length\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(y, y, color='blue', label='Predicted', alpha=0.5)\n",
    "plt.scatter(y, y_pred_all_unnormalized, color='red', label='Actual', alpha=0.5)  # Actual values\n",
    "plt.xlabel('Actual y')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title('Scatter Plot of Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse = 10\n",
    "best_model = 0\n",
    "\n",
    "alphas = [0, 0.1,0.5, 1,10,100]\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "\n",
    "    kf = KFold(n_splits=3, random_state=None)\n",
    "    total_test_mse = 0\n",
    "    total_train_mse = 0\n",
    "        \n",
    "    for j, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "        # Train Test split\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "\n",
    "        # Normalization\n",
    "        mean_X_train = np.mean(X_train, axis=0)\n",
    "        std_X_train = np.std(X_train, axis=0)\n",
    "        mean_y_train = np.mean(y_train)\n",
    "        std_y_train = np.std(y_train)\n",
    "\n",
    "\n",
    "        X_train_norm = (X_train - mean_X_train)/std_X_train\n",
    "        X_test_norm = (X_test - mean_X_train)/std_X_train\n",
    "\n",
    "        y_train_norm = (y_train - mean_y_train)/std_y_train\n",
    "        y_test_norm = (y_test - mean_y_train)/std_y_train\n",
    "\n",
    "        # Ridge Regression\n",
    "        model = Ridge(alpha = alpha)\n",
    "        model.fit(X_train_norm, y_train_norm)\n",
    "\n",
    "        # Train and test MSE\n",
    "        y_train_predict_unnormalized = (model.predict(X_train_norm) * std_y_train) + mean_y_train\n",
    "        y_test_predict_unnormalized = (model.predict(X_test_norm) * std_y_train) + mean_y_train\n",
    "\n",
    "        train_mse = np.mean((y_train_predict_unnormalized - y_train) ** 2)\n",
    "        test_mse = np.mean((y_test_predict_unnormalized - y_test) ** 2)\n",
    "        total_train_mse += train_mse\n",
    "        total_test_mse += test_mse\n",
    "\n",
    "        if (test_mse < best_mse):\n",
    "            best_mse = test_mse\n",
    "            best_model = model\n",
    "\n",
    "    print(\"Average Train MSE:\", total_train_mse / kf.get_n_splits())\n",
    "    print(\"Average Test MSE:\", total_test_mse / kf.get_n_splits())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_norm = (X - mean_X_train)/std_X_train\n",
    "y_pred_all = best_model.predict(X_all_norm)\n",
    "y_pred_all_unnormalized = (y_pred_all * std_y_train) + mean_y_train\n",
    "\n",
    "print (\"The r2 score is: \", r2_score(y,y_pred_all_unnormalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numss = [np.random.randint(1,len(y)) for _ in range(500)]\n",
    "y_rand = y.iloc[numss]\n",
    "y_pred_rand = y_pred_all_unnormalized[numss]\n",
    "\n",
    "# Assuming y_pred_all_unnormalized and y are numpy arrays of the same length\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(y, y, color='blue', label='Predicted', alpha=0.5)\n",
    "plt.scatter(y, y_pred_all_unnormalized, color='red', label='Actual', alpha=0.5)  # Actual values\n",
    "plt.xlabel('Actual y')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title('Scatter Plot of Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)\n",
    "best_model = 0\n",
    "best_mse = 10\n",
    "best_trainmse = 0\n",
    "# Normalization\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "std_X_train = np.std(X_train, axis=0)\n",
    "mean_y_train = np.mean(y_train)\n",
    "std_y_train = np.std(y_train)\n",
    "\n",
    "\n",
    "X_train_norm = (X_train - mean_X_train)/std_X_train\n",
    "X_test_norm = (X_test - mean_X_train)/std_X_train\n",
    "\n",
    "\n",
    "y_train_norm = (y_train - mean_y_train)/std_y_train\n",
    "y_test_norm = (y_test - mean_y_train)/std_y_train\n",
    "\n",
    "degrees = [1,2,4,8]\n",
    "alphas = [0.1,0.5,1,5,10]\n",
    "for deg in degrees:\n",
    "\n",
    "    # Polynomial Features\n",
    "    poly = PolynomialFeatures(degree=deg, include_bias= True)  # You can adjust the degree as needed\n",
    "    X_poly_train_norm = poly.fit_transform(X_train_norm)\n",
    "\n",
    "    for alpha in alphas:\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(X_poly_train_norm, y_train_norm)\n",
    "\n",
    "        X_poly_test_norm = poly.fit_transform(X_test_norm)\n",
    "\n",
    "\n",
    "        train_mse = mean_squared_error(y_train, (model.predict(X_poly_train_norm)*std_y_train)+mean_y_train)\n",
    "        test_mse = mean_squared_error(y_test, (model.predict(X_poly_test_norm)*std_y_train)+mean_y_train)\n",
    "        \n",
    "        if (test_mse < best_mse):\n",
    "            best_mse = test_mse\n",
    "            best_trainmse = train_mse\n",
    "            best_model = model\n",
    "            best_poly = poly\n",
    "\n",
    "print(\"Best model test MSE = \", best_mse)\n",
    "print(\"Best model train MSE = \", best_trainmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = (X - mean_X_train)/std_X_train\n",
    "X_poly = best_poly.fit_transform(X_norm)\n",
    "y_pred_all_unnormalized = (best_model.predict(X_poly)*std_y_train)+mean_y_train\n",
    "\n",
    "print (\"The r2 score is: \", r2_score(y,y_pred_all_unnormalized))\n",
    "\n",
    "numss = [np.random.randint(1,len(y)) for _ in range(500)]\n",
    "y_rand = y.iloc[numss]\n",
    "y_pred_rand = y_pred_all_unnormalized[numss]\n",
    "\n",
    "# Assuming y_pred_all_unnormalized and y are numpy arrays of the same length\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(y_rand, y_rand, color='blue', label='Predicted', alpha=0.5)\n",
    "plt.scatter(y_rand, y_pred_rand, color='red', label='Actual', alpha=0.5)  # Actual values\n",
    "plt.xlabel('Actual y')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title('Scatter Plot of Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print (\"The r2 score is: \", r2_score(y,y_pred_all_unnormalized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <strong> REMOVING OUTLIERS </strong> </p> <hr>\n",
    "<p> We used the <strong>Inter Quartile Range (IQR) method</strong> to filter out the outliers </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = 25\n",
    "\n",
    "first_quartile = np.percentile(y, change)\n",
    "fourth_quartile = np.percentile(y, 100-change)\n",
    "\n",
    "iqr = fourth_quartile - first_quartile\n",
    "\n",
    "lower_bound = first_quartile - 1.5 * iqr\n",
    "upper_bound = fourth_quartile + 1.5 * iqr\n",
    "\n",
    "outlier_indices = np.where((y < lower_bound) | (y > upper_bound))[0]\n",
    "\n",
    "merged_df_no_OUT = merged_df[(merged_df['gpa'] > lower_bound) & (merged_df['gpa'] < upper_bound)]\n",
    "\n",
    "\n",
    "X_no_outliers = merged_df_no_OUT[['SAT', 'athlete','hsize','hsrank','hsperc','female']]\n",
    "y_no_outliers = merged_df_no_OUT['gpa']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse = 10\n",
    "best_model = 0\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=3, random_state=None)\n",
    "total_test_mse = 0\n",
    "total_train_mse = 0\n",
    "    \n",
    "for j, (train_index, test_index) in enumerate(kf.split(X_no_outliers)):\n",
    "\n",
    "    # Train Test split\n",
    "    X_train, X_test = X_no_outliers.iloc[train_index], X_no_outliers.iloc[test_index]\n",
    "    y_train, y_test = y_no_outliers.iloc[train_index], y_no_outliers.iloc[test_index]\n",
    "\n",
    "\n",
    "    # Normalization\n",
    "    mean_X_train = np.mean(X_train, axis=0)\n",
    "    std_X_train = np.std(X_train, axis=0)\n",
    "    mean_y_train = np.mean(y_train)\n",
    "    std_y_train = np.std(y_train)\n",
    "\n",
    "\n",
    "    X_train_norm = (X_train - mean_X_train)/std_X_train\n",
    "    X_test_norm = (X_test - mean_X_train)/std_X_train\n",
    "\n",
    "    y_train_norm = (y_train - mean_y_train)/std_y_train\n",
    "    y_test_norm = (y_test - mean_y_train)/std_y_train\n",
    "\n",
    "    # Linear Regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_norm, y_train_norm)\n",
    "\n",
    "    # Train and test MSE\n",
    "    y_train_predict_unnormalized = (model.predict(X_train_norm) * std_y_train) + mean_y_train\n",
    "    y_test_predict_unnormalized = (model.predict(X_test_norm) * std_y_train) + mean_y_train\n",
    "\n",
    "    train_mse = np.mean((y_train_predict_unnormalized - y_train) ** 2)\n",
    "    test_mse = np.mean((y_test_predict_unnormalized - y_test) ** 2)\n",
    "    total_train_mse += train_mse\n",
    "    total_test_mse += test_mse\n",
    "\n",
    "    if (test_mse < best_mse):\n",
    "        best_mse = test_mse\n",
    "        best_model = model\n",
    "\n",
    "print(\"Average Train MSE:\", total_train_mse / kf.get_n_splits())\n",
    "print(\"Average Test MSE:\", total_test_mse / kf.get_n_splits())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_norm = (X_no_outliers - mean_X_train)/std_X_train\n",
    "y_pred_all = best_model.predict(X_all_norm)\n",
    "y_pred_all_unnormalized = (y_pred_all * std_y_train) + mean_y_train\n",
    "\n",
    "print (\"The r2 score is: \", r2_score(y_no_outliers,y_pred_all_unnormalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numss = [np.random.randint(1,len(y_no_outliers)) for _ in range(500)]\n",
    "y_rand = y_no_outliers.iloc[numss]\n",
    "y_pred_rand = y_pred_all_unnormalized[numss]\n",
    "\n",
    "# Assuming y_pred_all_unnormalized and y are numpy arrays of the same length\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(y_rand, y_rand, color='blue', label='Predicted', alpha=0.5)\n",
    "plt.scatter(y_rand, y_pred_rand, color='red', label='Actual', alpha=0.5)  # Actual values\n",
    "plt.xlabel('Actual y')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title('Scatter Plot of Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_no_outliers, y_no_outliers, test_size=0.2, random_state=50)\n",
    "best_model = 0\n",
    "best_mse = 10\n",
    "best_trainmse = 0\n",
    "\n",
    "# Normalization\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "std_X_train = np.std(X_train, axis=0)\n",
    "mean_y_train = np.mean(y_train)\n",
    "std_y_train = np.std(y_train)\n",
    "\n",
    "\n",
    "X_train_norm = (X_train - mean_X_train)/std_X_train\n",
    "X_test_norm = (X_test - mean_X_train)/std_X_train\n",
    "\n",
    "\n",
    "y_train_norm = (y_train - mean_y_train)/std_y_train\n",
    "y_test_norm = (y_test - mean_y_train)/std_y_train\n",
    "\n",
    "degrees = [1,2,4,8]\n",
    "alphas = [0.1,0.5,1,5,10]\n",
    "for deg in degrees:\n",
    "\n",
    "    # Polynomial Features\n",
    "    poly = PolynomialFeatures(degree=deg, include_bias= True)  # You can adjust the degree as needed\n",
    "    X_poly_train_norm = poly.fit_transform(X_train_norm)\n",
    "\n",
    "    for alpha in alphas:\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(X_poly_train_norm, y_train_norm)\n",
    "\n",
    "        X_poly_test_norm = poly.fit_transform(X_test_norm)\n",
    "\n",
    "\n",
    "        train_mse = mean_squared_error(y_train, (model.predict(X_poly_train_norm)*std_y_train)+mean_y_train)\n",
    "        test_mse = mean_squared_error(y_test, (model.predict(X_poly_test_norm)*std_y_train)+mean_y_train)\n",
    "        \n",
    "        if (test_mse < best_mse):\n",
    "            best_mse = test_mse\n",
    "            best_trainmse = train_mse\n",
    "            best_model = model\n",
    "            best_poly = poly\n",
    "\n",
    "print(\"Best model test MSE = \", best_mse)\n",
    "print(\"Best model train MSE = \", best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = (X_no_outliers - mean_X_train)/std_X_train\n",
    "X_poly = best_poly.fit_transform(X_norm)\n",
    "y_pred_all_unnormalized = (best_model.predict(X_poly)*std_y_train)+mean_y_train\n",
    "\n",
    "print (\"The r2 score is: \", r2_score(y_no_outliers,y_pred_all_unnormalized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numss = [np.random.randint(1,len(y_no_outliers)) for _ in range(500)]\n",
    "y_rand = y_no_outliers.iloc[numss]\n",
    "y_pred_rand = y_pred_all_unnormalized[numss]\n",
    "\n",
    "# Assuming y_pred_all_unnormalized and y are numpy arrays of the same length\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(y_rand, y_rand, color='blue', label='Predicted', alpha=0.5)\n",
    "plt.scatter(y_rand, y_pred_rand, color='red', label='Actual', alpha=0.5)  # Actual values\n",
    "plt.xlabel('Actual y')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title('Scatter Plot of Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start of <strong>Classification</strong> code cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df[['SAT', 'athlete','hsize','hsrank','female']] #Same concept here, dropping feature hs percentile as it is dependent on rank and size\n",
    "y = merged_df['Success']\n",
    "\n",
    "# Train-test split\n",
    "def KNNClassification(k):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n",
    "    model = KNeighborsClassifier(n_neighbors= k)\n",
    "\n",
    "    #Normalization\n",
    "    mean_X_train = np.mean(X_train, axis=0)\n",
    "    std_X_train = np.std(X_train, axis=0)\n",
    "\n",
    "\n",
    "    X_train_norm = (X_train-mean_X_train)/std_X_train\n",
    "\n",
    "\n",
    "    model.fit(X_train_norm,y_train)\n",
    "\n",
    "    X_test_norm = (X_test - mean_X_train)/ std_X_train\n",
    "\n",
    "\n",
    "    y_pred = model.predict(X_test_norm)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"Fail\", \"Success\"],)\n",
    "    cm_display.plot(cmap = \"YlOrRd\")\n",
    "    plt.show()\n",
    "\n",
    "    print(model.score(X_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionClassification(alphaValue):\n",
    "    model = LogisticRegression(random_state=0, C=alphaValue,fit_intercept=True)\n",
    "    model.fit(X_train_norm, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_norm)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"Fail\", \"Success\"],)\n",
    "    cm_display.plot(cmap = \"BuPu\")\n",
    "    plt.show()\n",
    "\n",
    "    print(model.score(X_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supportVectorClassifier():\n",
    "    model = SVC(kernel='rbf', probability=True)\n",
    "    model.fit(X_train_norm, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_norm)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [\"Fail\", \"Success\"],)\n",
    "    cm_display.plot(cmap = \"binary\")\n",
    "    plt.show()\n",
    "\n",
    "    print(model.score(X_test_norm, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard for Classification\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "fontColor = '#FFFFFF'\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "flag = 1\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\n",
    "        \"Classification Confusion Matrices Dashboard\",\n",
    "        style={'text-align': 'center'}\n",
    "    ),\n",
    "    dcc.Dropdown(\n",
    "        id='classification-model',\n",
    "        options=[\n",
    "            {'label': 'KNN', 'value': 'KNN'},\n",
    "            {'label': 'LogRegression', 'value': 'LogRegression'},\n",
    "            {'label': 'SVC', 'value': 'SVC'}\n",
    "        ],\n",
    "        value='KNN',\n",
    "        style={'width':'500px'}\n",
    "    ),\n",
    "    html.Div([\n",
    "        html.H5(\"N = \"),\n",
    "        dcc.Input(id='modelProp', value=5, type='text')\n",
    "    ], className='KNN-div',\n",
    "    style={'display': 'flex', 'flex-direction': 'row', 'gap': '10px','height':'30px','align-items':'center'}),\n",
    "\n",
    "    dcc.Graph(id='graph',style={'width':'700px'}),\n",
    "    html.Div(id='score')\n",
    "],style={'display':'flex','align-items':'center','justify-content':'center','flex-direction':'column','background-color':'white','gap':'10px'})\n",
    "\n",
    "@app.callback(\n",
    "    [Output('graph', 'figure'),\n",
    "    Output('score','children')],\n",
    "    [Input('classification-model', 'value'),\n",
    "     Input('modelProp','value')]\n",
    ")\n",
    "\n",
    "def update(model, modelProp):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n",
    "    n = 5\n",
    "\n",
    "    if model == 'KNN':\n",
    "        if modelProp != '':\n",
    "            n = int(modelProp)\n",
    "        mymodel = KNeighborsClassifier(n_neighbors=n)\n",
    "\n",
    "    elif model == 'LogRegression':\n",
    "        if modelProp != '':\n",
    "            n = float(modelProp)\n",
    "            if n != 0:\n",
    "                mymodel = LogisticRegression(random_state=0, C=n, fit_intercept=True)\n",
    "            else:\n",
    "                mymodel = LogisticRegression(random_state=0, C=0.01, fit_intercept=True)\n",
    "        else:\n",
    "            mymodel = LogisticRegression(random_state=0, C=0.01, fit_intercept=True)\n",
    "\n",
    "    elif model == 'SVC':\n",
    "        mymodel = SVC(kernel='rbf', probability=True)\n",
    "\n",
    "    # Normalization\n",
    "    mean_X_train = np.mean(X_train, axis=0)\n",
    "    std_X_train = np.std(X_train, axis=0)\n",
    "\n",
    "    X_train_norm = (X_train - mean_X_train) / std_X_train\n",
    "\n",
    "    mymodel.fit(X_train_norm, y_train)\n",
    "\n",
    "    X_test_norm = (X_test - mean_X_train) / std_X_train\n",
    "\n",
    "    y_pred = mymodel.predict(X_test_norm)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    x_values = [\"Fail\", \"Success\"]\n",
    "    y_values = x_values[::].copy()\n",
    "    cm_text = [[str(y_values) for y_values in x_values] for x_values in cm]\n",
    "    fig = ff.create_annotated_heatmap(cm, x=x_values, y=y_values, annotation_text=cm_text, colorscale='Viridis')\n",
    "\n",
    "    fig.update_layout(title_text='<b>Confusion Matrix</b>')\n",
    "\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\", size=14),\n",
    "                            x=0.5,\n",
    "                            y=-0.15,\n",
    "                            showarrow=False,\n",
    "                            text=\"Predicted value\",\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"))\n",
    "\n",
    "    # add custom yaxis title\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\", size=14),\n",
    "                            x=-0.35,\n",
    "                            y=0.5,\n",
    "                            showarrow=False,\n",
    "                            text=\"Real value\",\n",
    "                            textangle=-90,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"))\n",
    "    \n",
    "    # adjust margins to make room for yaxis title\n",
    "    fig.update_layout(margin=dict(t=50, l=200))\n",
    "    \n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    Recall = tp/(tp+fn)\n",
    "    Precision = tp/(tp+fp)\n",
    "    Accuracy = (tp+tn)/(tp+fn+tn+fp)\n",
    "    F_measure = (2*Recall*Precision)/(Recall+Precision)\n",
    "\n",
    "    return fig,Accuracy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
